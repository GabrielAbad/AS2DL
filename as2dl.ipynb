{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2118595,"sourceType":"datasetVersion","datasetId":1271215}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Preparação do Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nfrom PIL import Image\nimport shutil\n\n# Define dataset root\noriginal_dataset_path = '/kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/'\nyolo_dataset_path = 'yolo_dataset'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T12:11:23.421061Z","iopub.execute_input":"2025-09-15T12:11:23.421327Z","iopub.status.idle":"2025-09-15T12:11:23.425489Z","shell.execute_reply.started":"2025-09-15T12:11:23.421307Z","shell.execute_reply":"2025-09-15T12:11:23.424766Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"yolo_dirs = [\n    os.path.join(yolo_dataset_path, 'images', 'train'),\n    os.path.join(yolo_dataset_path, 'images', 'val'),\n    os.path.join(yolo_dataset_path, 'labels', 'train'),\n    os.path.join(yolo_dataset_path, 'labels', 'val')\n]\n\nfor yolo_dir in yolo_dirs:\n    os.makedirs(yolo_dir, exist_ok=True)\n\njpeg_images_dir = os.path.join(original_dataset_path, 'VOC2012_train_val', 'JPEGImages')\nannotations_dir = os.path.join(original_dataset_path, 'VOC2012_train_val', 'Annotations')\nif not os.path.exists(jpeg_images_dir) or not os.path.exists(annotations_dir):\n    raise FileNotFoundError(f\"The directory {jpeg_images_dir} or {annotations_dir} does not exist. Please verify the dataset path.\")\nimage_filenames = os.listdir(jpeg_images_dir)\nimage_ids = [os.path.splitext(filename)[0] for filename in image_filenames if filename.endswith('.jpg')]\n\nrandom.seed(42)\nrandom.shuffle(image_ids)\nsplit_index = int(0.8 * len(image_ids)) #Spliting the dataset 80% for training, 20% for validation\ntrain_ids = image_ids[:split_index] #taking the first 80% pictures\nval_ids = image_ids[split_index:]\nprint(len(train_ids))\nprint(len(val_ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T12:11:25.672462Z","iopub.execute_input":"2025-09-15T12:11:25.673347Z","iopub.status.idle":"2025-09-15T12:11:25.853458Z","shell.execute_reply.started":"2025-09-15T12:11:25.673315Z","shell.execute_reply":"2025-09-15T12:11:25.852635Z"}},"outputs":[{"name":"stdout","text":"13700\n3425\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import xml.etree.ElementTree as ET \n#this fucntion converts PASCAL_VOC annotations to YOLO format\ndef create_yolo_annotation(xml_file_path, yolo_label_path, label_dict):\n    tree = ET.parse(xml_file_path)\n    root = tree.getroot()\n    annotations = [] #list that will store the converted YOLO annotations.\n\n    img_width = int(root.find('size/width').text)\n    img_height = int(root.find('size/height').text)\n\n    for obj in root.findall('object'):\n        label = obj.find('name').text\n        if label not in label_dict:\n            continue\n        label_idx = label_dict[label]\n        bndbox = obj.find('bndbox')\n        xmin = float(bndbox.find('xmin').text)\n        ymin = float(bndbox.find('ymin').text)\n        xmax = float(bndbox.find('xmax').text)\n        ymax = float(bndbox.find('ymax').text)\n\n        # this is YOLOv8 annotation format: label x_center y_center width height (normalized)\n        x_center = ((xmin + xmax) / 2) / img_width\n        y_center = ((ymin + ymax) / 2) / img_height\n        width = (xmax - xmin) / img_width\n        height = (ymax - ymin) / img_height\n\n        annotations.append(f\"{label_idx} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n\n    #annotations to the label file\n    with open(yolo_label_path, 'w') as f:\n        f.write(\"\\n\".join(annotations))\n\nlabel_dict = {\n    'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n    'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9,\n    'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14,\n    'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19\n}\n\nfor image_set, ids in [('train', train_ids), ('val', val_ids)]:\n    for img_id in ids:\n        img_src_path = os.path.join(jpeg_images_dir, f'{img_id}.jpg')\n        label_dst_path = os.path.join(yolo_dataset_path, 'labels', image_set, f'{img_id}.txt')\n\n        # Create the YOLO annotation file\n        xml_file_path = os.path.join(annotations_dir, f'{img_id}.xml')\n        if not os.path.exists(xml_file_path):\n            print(f\"Warning: Annotation {xml_file_path} not found, skipping.\")\n            continue\n        create_yolo_annotation(xml_file_path, label_dst_path, label_dict)\n\n        # Copy the image to the new YOLO dataset structure\n        img_dst_path = os.path.join(yolo_dataset_path, 'images', image_set, f'{img_id}.jpg')\n        shutil.copy(img_src_path, img_dst_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T12:11:27.893238Z","iopub.execute_input":"2025-09-15T12:11:27.893753Z","iopub.status.idle":"2025-09-15T12:14:33.162315Z","shell.execute_reply.started":"2025-09-15T12:11:27.893728Z","shell.execute_reply":"2025-09-15T12:14:33.161562Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 1. Criar o arquivo de nomes das classes (voc.names)\nnames_file_path = os.path.join(yolo_dataset_path, 'voc.names')\nwith open(names_file_path, 'w') as f:\n    for label in label_dict.keys():\n        f.write(f\"{label}\\n\")\nprint(f\"Arquivo 'voc.names' criado em: {names_file_path}\")\n\n\n# 2. Criar os arquivos de lista de imagens (train.txt e val.txt) com caminhos absolutos\ntrain_txt_path = os.path.join(yolo_dataset_path, 'train.txt')\nval_txt_path = os.path.join(yolo_dataset_path, 'val.txt')\n\n# Para o conjunto de treino\nwith open(train_txt_path, 'w') as f:\n    for img_id in train_ids:\n        img_path = os.path.abspath(os.path.join(yolo_dataset_path, 'images', 'train', f'{img_id}.jpg'))\n        f.write(f\"{img_path}\\n\")\nprint(f\"Arquivo 'train.txt' criado em: {train_txt_path}\")\n\n# Para o conjunto de validação\nwith open(val_txt_path, 'w') as f:\n    for img_id in val_ids:\n        img_path = os.path.abspath(os.path.join(yolo_dataset_path, 'images', 'val', f'{img_id}.jpg'))\n        f.write(f\"{img_path}\\n\")\nprint(f\"Arquivo 'val.txt' criado em: {val_txt_path}\")\n\n\n# 3. Criar o arquivo de configuração de dados (voc.data)\ndata_config_path = os.path.join(yolo_dataset_path, 'voc.data')\nnum_classes = len(label_dict)\n\nwith open(data_config_path, 'w') as f:\n    f.write(f\"classes = {num_classes}\\n\")\n    f.write(f\"train = {os.path.abspath(train_txt_path)}\\n\")\n    f.write(f\"valid = {os.path.abspath(val_txt_path)}\\n\")\n    f.write(f\"names = {os.path.abspath(names_file_path)}\\n\")\n    f.write(\"backup = backup/\") # Pasta para salvar os pesos durante o treino\nprint(f\"Arquivo 'voc.data' criado em: {data_config_path}\")\n\n# Cria a pasta de backup, se não existir\nos.makedirs(os.path.join(yolo_dataset_path, 'backup'), exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T12:14:33.163522Z","iopub.execute_input":"2025-09-15T12:14:33.163737Z","iopub.status.idle":"2025-09-15T12:14:33.249220Z","shell.execute_reply.started":"2025-09-15T12:14:33.163719Z","shell.execute_reply":"2025-09-15T12:14:33.248637Z"}},"outputs":[{"name":"stdout","text":"Arquivo 'voc.names' criado em: yolo_dataset/voc.names\nArquivo 'train.txt' criado em: yolo_dataset/train.txt\nArquivo 'val.txt' criado em: yolo_dataset/val.txt\nArquivo 'voc.data' criado em: yolo_dataset/voc.data\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Configuração e Utilitários","metadata":{}},{"cell_type":"code","source":"# Instalações e Importações\n# =======================================\n\nimport torch\nimport torch.nn as nn\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T12:15:06.778357Z","iopub.execute_input":"2025-09-15T12:15:06.778685Z","iopub.status.idle":"2025-09-15T12:15:10.744655Z","shell.execute_reply.started":"2025-09-15T12:15:06.778660Z","shell.execute_reply":"2025-09-15T12:15:10.744026Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Configuração Principal do Projeto\n# =============================================\n\n# -- Dispositivo --\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -- Caminhos do Dataset --\nDATASET_PATH = \"yolo_dataset\" \nTRAIN_CSV_PATH = os.path.join(DATASET_PATH, \"train.txt\")\nVAL_CSV_PATH = os.path.join(DATASET_PATH, \"val.txt\")\n\n# -- Hiperparâmetros de Treinamento --\nLEARNING_RATE = 2e-5\nBATCH_SIZE = 8\nNUM_WORKERS = 4 # Kaggle geralmente oferece 4 cores, então 4 é um bom valor\nNUM_EPOCHS = 100 # Número de épocas para treinar\nPIN_MEMORY = True # Otimização para carregar dados mais rápido para a GPU\nLOAD_MODEL = False # Se True, carrega um modelo pré-treinado\nSAVE_MODEL = True # Se True, salva o modelo durante o treinamento\n\n# -- Constantes do Modelo e Dataset --\nIMAGE_SIZE = 416 # Tamanho padrão de entrada para YOLOv3\nNUM_CLASSES = 20 # 20 classes para o PASCAL VOC\nCONF_THRESHOLD = 0.6 # Limite de confiança para uma predição ser considerada\nMAP_IOU_THRESH = 0.5 # Limite de IoU para cálculo do mAP (True Positive)\nNMS_IOU_THRESH = 0.45 # Limite de IoU para o Non-Maximum Suppression\n\n# -- Âncoras (Anchors) da YOLOv3 --\n# Estas âncoras são pré-calculadas e são padrão para YOLOv3.\n# Estão agrupadas por escala de predição.\n# Formato: [(largura_ancora_1, altura_ancora_1), (largura_ancora_2, altura_ancora_2), ...]\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],      # Para a grade de predição GRANDE (detecta objetos pequenos)\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],     # Para a grade de predição MÉDIA (detecta objetos médios)\n    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],     # Para a grade de predição PEQUENA (detecta objetos grandes)\n]\n\n# -- Checkpoints para salvar/carregar o modelo --\nCHECKPOINT_FILE = \"yolov3_voc.pth.tar\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T12:15:10.745800Z","iopub.execute_input":"2025-09-15T12:15:10.746150Z","iopub.status.idle":"2025-09-15T12:15:10.808007Z","shell.execute_reply.started":"2025-09-15T12:15:10.746123Z","shell.execute_reply":"2025-09-15T12:15:10.807335Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Função Utilitária - Intersection over Union (IoU)\n# =============================================================\ndef iou(box1, box2, is_pred=True):\n    \"\"\"\n    Calcula a Interseção sobre União (IoU) entre duas caixas delimitadoras ou apenas (w,h).\n    \"\"\"\n    if is_pred:\n        # box1 e box2 no formato (x, y, w, h)\n        b1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n        b1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n        b1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n        b1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n\n        b2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n        b2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n        b2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n        b2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n    else:\n        # box1 = (w, h), box2 = lista de âncoras (w, h)\n        b1_w, b1_h = box1[0], box1[1]\n        b2_w, b2_h = box2[:, 0], box2[:, 1]\n\n        intersection = torch.min(b1_w, b2_w) * torch.min(b1_h, b2_h)\n        union = (b1_w * b1_h) + (b2_w * b2_h) - intersection + 1e-6\n        return intersection / union\n\n    inter_x1 = torch.max(b1_x1, b2_x1)\n    inter_y1 = torch.max(b1_y1, b2_y1)\n    inter_x2 = torch.min(b1_x2, b2_x2)\n    inter_y2 = torch.min(b1_y2, b2_y2)\n\n    intersection_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n    box1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\n    box2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1))\n    union_area = box1_area + box2_area - intersection_area + 1e-6\n\n    return intersection_area / union_area\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:08:19.937121Z","iopub.execute_input":"2025-09-15T13:08:19.937708Z","iopub.status.idle":"2025-09-15T13:08:19.945965Z","shell.execute_reply.started":"2025-09-15T13:08:19.937678Z","shell.execute_reply":"2025-09-15T13:08:19.945328Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Função Utilitária - Non-Maximum Suppression (NMS)\n# ==============================================================\n\ndef nms(bboxes, iou_threshold, threshold):\n    \"\"\"\n    Executa Non-Maximum Suppression para filtrar caixas delimitadoras.\n\n    Parâmetros:\n        bboxes (list): Lista de predições. Cada predição é uma lista no formato\n                       [class_id, confidence_score, x1, y1, x2, y2].\n        iou_threshold (float): Limite de IoU para suprimir caixas.\n        threshold (float): Limite de confiança para considerar uma caixa.\n\n    Retorna:\n        list: A lista de caixas após aplicar NMS.\n    \"\"\"\n    \n    # 1. Filtra as caixas com score de confiança abaixo do threshold\n    bboxes = [box for box in bboxes if box[1] > threshold]\n\n    # 2. Ordena as caixas pela confiança, da maior para a menor\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    \n    bboxes_after_nms = []\n\n    while bboxes:\n        # 3. Pega a caixa com a maior confiança\n        chosen_box = bboxes.pop(0)\n        bboxes_after_nms.append(chosen_box)\n\n        # 4. Compara a caixa escolhida com todas as outras restantes\n        bboxes = [\n            box\n            for box in bboxes\n            # Mantém apenas as caixas que são de outra classe\n            if box[0] != chosen_box[0]\n            # Ou que têm IoU abaixo do threshold com a caixa escolhida\n            or iou(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                is_pred=False\n            ) < iou_threshold\n        ]\n\n    return bboxes_after_nms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T12:15:49.914548Z","iopub.execute_input":"2025-09-15T12:15:49.914845Z","iopub.status.idle":"2025-09-15T12:15:49.920422Z","shell.execute_reply.started":"2025-09-15T12:15:49.914824Z","shell.execute_reply":"2025-09-15T12:15:49.919692Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Preparação e Carregamento dos Dados","metadata":{}},{"cell_type":"code","source":"# Instalação Adicional e Importações\n# ==========================================================\n\n!pip install -q albumentations==1.3.1 opencv-python-headless\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T12:22:45.266564Z","iopub.execute_input":"2025-09-15T12:22:45.267362Z","iopub.status.idle":"2025-09-15T12:22:49.007417Z","shell.execute_reply.started":"2025-09-15T12:22:45.267335Z","shell.execute_reply":"2025-09-15T12:22:49.006435Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"class YOLOv3Dataset(Dataset):\n    def __init__(\n        self,\n        txt_path,\n        anchors,\n        image_size=416,\n        S=[13, 26, 52], \n        C=20,\n        transform=None,\n    ):\n        with open(txt_path, \"r\") as f:\n            self.annotations = [line.strip() for line in f.readlines() if line.strip()]\n\n        self.image_size = image_size\n        self.transform = transform\n        self.S = S\n        self.C = C\n        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n        self.num_anchors = self.anchors.shape[0]\n        self.num_anchors_per_scale = self.num_anchors // 3\n        self.ignore_iou_thresh = 0.5\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        image_path = self.annotations[index]\n        label_path = image_path.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n    \n        # Carrega a imagem\n        image = np.array(Image.open(image_path).convert(\"RGB\"))\n    \n        # Inicializa os targets\n        targets = [torch.zeros((self.num_anchors_per_scale, S, S, 6)) for S in self.S]\n        bboxes = np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2)\n        bboxes = np.roll(bboxes, shift=-1, axis=1)  # move class para o fim\n    \n        # Aplica data augmentation\n        if self.transform:\n            if len(bboxes) == 0:\n                bboxes_for_transform = np.empty((0, 5))\n            else:\n                bboxes_for_transform = bboxes\n    \n            augmentations = self.transform(image=image, bboxes=bboxes_for_transform)\n            image = augmentations[\"image\"]\n            bboxes = augmentations[\"bboxes\"]\n    \n        for box in bboxes:\n            x, y, w, h, class_label = box\n\n            iou_anchors = iou(torch.tensor([w, h]), self.anchors, is_pred=False)\n    \n            anchor_indices = iou_anchors.argsort(descending=True)\n            best_anchor_idx = anchor_indices[0]\n    \n            scale_idx = (best_anchor_idx // self.num_anchors_per_scale).item()\n            anchor_on_scale = (best_anchor_idx % self.num_anchors_per_scale).item()\n    \n            S = self.S[scale_idx]\n            i, j = int(S * y), int(S * x)\n    \n            target_scale = targets[scale_idx]\n            if target_scale[anchor_on_scale, i, j, 0] == 0:\n                target_scale[anchor_on_scale, i, j, 0] = 1\n                x_cell, y_cell = S * x - j, S * y - i\n                w_cell, h_cell = w * S, h * S\n                box_coordinates = torch.tensor([x_cell, y_cell, w_cell, h_cell])\n                target_scale[anchor_on_scale, i, j, 1:5] = box_coordinates\n                target_scale[anchor_on_scale, i, j, 5] = int(class_label)\n    \n            for anchor_idx in anchor_indices[1:]:\n                if iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n                    scale_idx_ignore = (anchor_idx // self.num_anchors_per_scale).item()\n                    anchor_on_scale_ignore = (anchor_idx % self.num_anchors_per_scale).item()\n    \n                    S_ignore = self.S[scale_idx_ignore]\n                    i_ignore, j_ignore = int(S_ignore * y), int(S_ignore * x)\n    \n                    if targets[scale_idx_ignore][anchor_on_scale_ignore, i_ignore, j_ignore, 0] == 0:\n                        targets[scale_idx_ignore][anchor_on_scale_ignore, i_ignore, j_ignore, 0] = -1\n    \n        return image, tuple(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:15:43.252603Z","iopub.execute_input":"2025-09-15T13:15:43.252934Z","iopub.status.idle":"2025-09-15T13:15:43.265763Z","shell.execute_reply.started":"2025-09-15T13:15:43.252908Z","shell.execute_reply":"2025-09-15T13:15:43.265053Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"## Arquitetura da YOLO","metadata":{}},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.leaky = nn.LeakyReLU(0.1)\n        self.use_bn_act = bn_act\n\n    def forward(self, x):\n        if self.use_bn_act:\n            return self.leaky(self.bn(self.conv(x)))\n        else:\n            return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:23:29.263049Z","iopub.execute_input":"2025-09-15T13:23:29.263314Z","iopub.status.idle":"2025-09-15T13:23:29.268298Z","shell.execute_reply.started":"2025-09-15T13:23:29.263295Z","shell.execute_reply":"2025-09-15T13:23:29.267529Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, channels, use_residual=True, num_repeats=1):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        for _ in range(num_repeats):\n            self.layers += [\n                nn.Sequential(\n                    CNNBlock(channels, channels // 2, kernel_size=1),\n                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n                )\n            ]\n        self.use_residual = use_residual\n        self.num_repeats = num_repeats\n\n    def forward(self, x):\n        for layer in self.layers:\n            if self.use_residual:\n                x = x + layer(x)\n            else:\n                x = layer(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:23:32.651156Z","iopub.execute_input":"2025-09-15T13:23:32.651426Z","iopub.status.idle":"2025-09-15T13:23:32.657104Z","shell.execute_reply.started":"2025-09-15T13:23:32.651404Z","shell.execute_reply":"2025-09-15T13:23:32.656293Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"class ScalePrediction(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.pred = nn.Sequential(\n            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n            CNNBlock(\n                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n            ),\n        )\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        # Transforma a saída do formato (N, C, H, W) para o formato YOLO\n        # N -> Batch Size\n        # C -> 3 * (5 + num_classes)\n        # H, W -> Dimensões da grade (S)\n        # Saída desejada: (N, 3, S, S, 5 + num_classes)\n        return (\n            self.pred(x)\n            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n            .permute(0, 1, 3, 4, 2)\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:24:23.750164Z","iopub.execute_input":"2025-09-15T13:24:23.750872Z","iopub.status.idle":"2025-09-15T13:24:23.755650Z","shell.execute_reply.started":"2025-09-15T13:24:23.750845Z","shell.execute_reply":"2025-09-15T13:24:23.754871Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Configuração da arquitetura YOLOv3\n# Tupla: (out_channels, kernel_size, stride)\n# Lista: [\"B\", num_repeats] -> Bloco Residual\n# \"S\": Scale Prediction\n# \"U\": Upsampling\nconfig = [\n    (32, 3, 1),\n    (64, 3, 2),\n    [\"B\", 1],\n    (128, 3, 2),\n    [\"B\", 2],\n    (256, 3, 2),\n    [\"B\", 8],\n    (512, 3, 2),\n    [\"B\", 8],\n    (1024, 3, 2),\n    [\"B\", 4],  # Fim da Darknet-53\n    (512, 1, 1),\n    (1024, 3, 1),\n    \"S\", # Predição em Escala 1 (grade 13x13)\n    (256, 1, 1),\n    \"U\",\n    (256, 1, 1),\n    (512, 3, 1),\n    \"S\", # Predição em Escala 2 (grade 26x26)\n    (128, 1, 1),\n    \"U\",\n    (128, 1, 1),\n    (256, 3, 1),\n    \"S\", # Predição em Escala 3 (grade 52x52)\n]\n\n\nclass YOLOv3(nn.Module):\n    def __init__(self, in_channels=3, num_classes=20):\n        super().__init__()\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        self.layers = self._create_conv_layers()\n\n    def forward(self, x):\n        outputs = []\n        route_connections = []\n        for layer in self.layers:\n            if isinstance(layer, ScalePrediction):\n                outputs.append(layer(x))\n                continue\n\n            x = layer(x)\n\n            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n                route_connections.append(x)\n            \n            elif isinstance(layer, nn.Upsample):\n                x = torch.cat([x, route_connections.pop()], dim=1)\n        \n        return outputs\n\n    def _create_conv_layers(self):\n        layers = nn.ModuleList()\n        in_channels = self.in_channels\n\n        for module in config:\n            if isinstance(module, tuple):\n                out_channels, kernel_size, stride = module\n                layers.append(\n                    CNNBlock(\n                        in_channels,\n                        out_channels,\n                        kernel_size=kernel_size,\n                        stride=stride,\n                        padding=1 if kernel_size == 3 else 0,\n                    )\n                )\n                in_channels = out_channels\n\n            elif isinstance(module, list):\n                num_repeats = module[1]\n                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats))\n\n            elif isinstance(module, str):\n                if module == \"S\":\n                    layers += [\n                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n                    ]\n                    in_channels = in_channels // 2\n\n                elif module == \"U\":\n                    layers.append(nn.Upsample(scale_factor=2))\n                    in_channels = in_channels * 3 # Concatenação com a route connection\n        \n        return layers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:25:21.859339Z","iopub.execute_input":"2025-09-15T13:25:21.860046Z","iopub.status.idle":"2025-09-15T13:25:21.870337Z","shell.execute_reply.started":"2025-09-15T13:25:21.860010Z","shell.execute_reply":"2025-09-15T13:25:21.869537Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"## Loss","metadata":{}},{"cell_type":"code","source":"class YOLOLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.cross_entropy = nn.CrossEntropyLoss()\n        self.sigmoid = nn.Sigmoid()\n\n        # Usando os lambdas que você tinha no seu código\n        self.lambda_class = 1\n        self.lambda_noobj = 1\n        self.lambda_obj = 1\n        self.lambda_box = 1\n\n    def forward(self, predictions, targets, anchors):\n        total_loss = 0\n\n        for i in range(3):\n            prediction = predictions[i]\n            target = targets[i]\n            anchors_scale = anchors[i]\n\n            obj = target[..., 0] == 1\n            noobj = target[..., 0] == 0\n\n            # --- 1. Perda de \"Não Objeto\" (No Object Loss) ---\n            no_obj_loss = self.bce(\n                (prediction[..., 0:1][noobj]), (target[..., 0:1][noobj])\n            )\n\n            # --- 2. Perda de Objeto (Object Loss) ---\n            anchors_scale_reshaped = anchors_scale.reshape(1, 3, 1, 1, 2)\n            box_preds = torch.cat([self.sigmoid(prediction[..., 1:3]), torch.exp(prediction[..., 3:5]) * anchors_scale_reshaped], dim=-1)\n            ious = iou(box_preds[obj], target[..., 1:5][obj], is_pred=True).detach()\n            object_loss = self.bce(\n                (prediction[..., 0:1][obj]), (ious * target[..., 0:1][obj])\n            )\n\n            # --- 3. Perda da Caixa Delimitadora ---\n            t_xy = self.sigmoid(target[..., 1:3])\n            t_wh = torch.log(\n                (1e-16 + target[..., 3:5]) / anchors_scale_reshaped\n            )\n            box_loss_target = torch.cat((t_xy, t_wh), dim=-1)\n\n            # Calcula a perda MSE apenas nas posições onde há objetos.\n            box_loss = self.mse(prediction[..., 1:5][obj], box_loss_target[obj])\n\n\n            # --- 4. Perda de Classificação (Class Loss) ---\n            class_loss = self.cross_entropy(\n                (prediction[..., 5:][obj]), (target[..., 5][obj].long()),\n            )\n\n            # Evita adicionar perdas NaN se não houver objetos na amostra\n            # (object_loss, box_loss, class_loss podem ser NaN se 'obj' estiver vazio)\n            if torch.isnan(object_loss): object_loss = 0\n            if torch.isnan(box_loss): box_loss = 0\n            if torch.isnan(class_loss): class_loss = 0\n                \n            total_loss += (\n                self.lambda_box * box_loss\n                + self.lambda_obj * object_loss\n                + self.lambda_noobj * no_obj_loss\n                + self.lambda_class * class_loss\n            )\n\n        return total_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T14:24:24.772982Z","iopub.execute_input":"2025-09-15T14:24:24.773554Z","iopub.status.idle":"2025-09-15T14:24:24.783694Z","shell.execute_reply.started":"2025-09-15T14:24:24.773520Z","shell.execute_reply":"2025-09-15T14:24:24.782924Z"}},"outputs":[],"execution_count":105},{"cell_type":"markdown","source":"## Treino","metadata":{}},{"cell_type":"code","source":"# Funções Utilitárias (Checkpoints, Conversão de Saída e mAP)\n# =======================================================================\nfrom collections import Counter\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndef save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Salvando checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Carregando checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n\ndef convert_cells_to_bboxes(predictions, anchors, S, is_predictions=True):\n    # predictions: (N, 3, S, S, 5+C)\n    # anchors: (3, 2)\n    # S: grid_size\n    batch_size = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n\n    if is_predictions:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n\n    cell_indices = (\n        torch.arange(S)\n        .repeat(predictions.shape[0], 3, S, 1)\n        .unsqueeze(-1)\n        .to(predictions.device)\n    )\n    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 / S * box_predictions[..., 2:4]\n    \n    converted_bboxes = torch.cat((x, y, w_h), dim=-1).reshape(batch_size, num_anchors * S * S, 4)\n    return converted_bboxes.tolist()\n\ndef get_evaluation_bboxes(loader, model, iou_threshold, anchors, threshold, device):\n    model.eval()\n    train_idx = 0\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    for batch_idx, (x, labels) in enumerate(tqdm(loader, desc=\"Calculando predições para mAP\")):\n        x = x.to(device)\n        with torch.no_grad():\n            with torch.amp.autocast(device_type=device, dtype=torch.float16):\n                predictions = model(x)\n\n        batch_size = x.shape[0]\n        bboxes = [[] for _ in range(batch_size)]\n        \n        for i in range(3): # 3 scales\n            S = predictions[i].shape[2]\n            anchor = torch.tensor([*anchors[i]]).to(device) * S\n            boxes_scale_i = convert_cells_to_bboxes(predictions[i], anchor, S=S, is_predictions=True)\n            for idx, (box) in enumerate(boxes_scale_i):\n                bboxes[idx] += box\n\n        # Processar ground truths\n        true_bboxes = convert_cells_to_bboxes(labels[0], anchors[0], S=13, is_predictions=False)\n        for idx in range(batch_size):\n            all_pred_boxes.append(bboxes[idx])\n            all_true_boxes.append(true_bboxes[idx])\n            \n    model.train()\n    return all_pred_boxes, all_true_boxes\n\ndef mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20):\n    # pred_boxes (list): [[train_idx, class_pred, prob_score, x, y, w, h], ...]\n    average_precisions = []\n    epsilon = 1e-6\n\n    for c in range(num_classes):\n        detections = [det for det in pred_boxes if det[1] == c]\n        ground_truths = [gt for gt in true_boxes if gt[1] == c]\n\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n        \n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n            best_iou = 0\n            best_gt_idx = -1\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou_val = iou(torch.tensor(detection[3:]), torch.tensor(gt[3:]), is_pred=True)\n                if iou_val > best_iou:\n                    best_iou = iou_val\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T14:31:16.472571Z","iopub.execute_input":"2025-09-15T14:31:16.473231Z","iopub.status.idle":"2025-09-15T14:31:16.494129Z","shell.execute_reply.started":"2025-09-15T14:31:16.473208Z","shell.execute_reply":"2025-09-15T14:31:16.493240Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"# A Função de Treinamento (train_fn)\n# ===============================================\n\ndef train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n    loop = tqdm(train_loader, leave=True)\n    losses = []\n\n    for batch_idx, (x, y) in enumerate(loop):\n        x = x.to(DEVICE)\n        y0, y1, y2 = (\n            y[0].to(DEVICE),\n            y[1].to(DEVICE),\n            y[2].to(DEVICE),\n        )\n\n        with torch.amp.autocast(device_type=DEVICE, dtype=torch.float16):\n            out = model(x)\n            loss = loss_fn(out, (y0, y1, y2), scaled_anchors)\n\n        losses.append(loss.item())\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # Atualiza a barra de progresso\n        mean_loss = sum(losses) / len(losses)\n        loop.set_postfix(loss=mean_loss)\n        \n    return mean_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T14:32:04.007295Z","iopub.execute_input":"2025-09-15T14:32:04.007635Z","iopub.status.idle":"2025-09-15T14:32:04.014304Z","shell.execute_reply.started":"2025-09-15T14:32:04.007581Z","shell.execute_reply":"2025-09-15T14:32:04.013496Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"model = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\nloss_fn = YOLOLoss()\nscaler = torch.amp.GradScaler()\n\n# --- CONFIGURAÇÃO DOS DATALOADERS ---\ntrain_transform = A.Compose(\n    [\n        A.LongestMaxSize(max_size=IMAGE_SIZE),\n        A.PadIfNeeded(min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=0),\n        A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5, p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n        ToTensorV2(),\n    ],\n    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n)\nval_transform = A.Compose(\n    [\n        A.LongestMaxSize(max_size=IMAGE_SIZE),\n        A.PadIfNeeded(min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=0),\n        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n        ToTensorV2(),\n    ],\n    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n)\nBATCH_SIZE = 16\ntrain_dataset = YOLOv3Dataset(txt_path=TRAIN_CSV_PATH, anchors=ANCHORS, transform=train_transform)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n\nval_dataset = YOLOv3Dataset(txt_path=VAL_CSV_PATH, anchors=ANCHORS, transform=val_transform)\nval_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Prepara as âncoras para a função de perda\nscaled_anchors = (\n    torch.tensor(ANCHORS) * torch.tensor([IMAGE_SIZE, IMAGE_SIZE]).view(1, 1, 2)\n).to(DEVICE)\nscaled_anchors = scaled_anchors.view(3,3,2)\n\n# Lista para armazenar a perda de cada época para o gráfico\nepoch_losses = []\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\n--- Época {epoch+1} / {NUM_EPOCHS} ---\")\n    \n    mean_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n    epoch_losses.append(mean_loss)\n    \n    # Salva o checkpoint ao final de cada época\n    if SAVE_MODEL:\n        save_checkpoint(model, optimizer, filename=CHECKPOINT_FILE)\n\nprint(\"\\nTreinamento concluído!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T14:54:16.922999Z","iopub.execute_input":"2025-09-15T14:54:16.923551Z","iopub.status.idle":"2025-09-15T14:54:28.661156Z","shell.execute_reply.started":"2025-09-15T14:54:16.923525Z","shell.execute_reply":"2025-09-15T14:54:28.659909Z"}},"outputs":[{"name":"stdout","text":"\n--- Época 1 / 100 ---\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 27/857 [00:11<05:41,  2.43it/s, loss=25.1]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2400175687.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Época {epoch+1} / {NUM_EPOCHS} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmean_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_anchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/3561915241.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_anchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/3922954951.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets, anchors)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# --- 1. Perda de \"Não Objeto\" (No Object Loss) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             no_obj_loss = self.bce(\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":113},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(range(1, 20 + 1), epoch_losses, marker='o', linestyle='-')\nplt.title('Curva de Perda Durante o Treinamento', fontsize=16)\nplt.xlabel('Época', fontsize=12)\nplt.ylabel('Perda Média (Mean Loss)', fontsize=12)\nplt.grid(True)\nplt.xticks(range(1, 20 + 1))\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Avaliação Final - Cálculo do mAP\n# ============================================\n\nmodel.to(DEVICE)\n\nprint(\"=> Carregando modelo treinado para avaliação final...\")\nload_checkpoint(\"/kaggle/output/yolov3_voc.pth.tar\", model, optimizer, lr=LEARNING_RATE)\n\nprint(\"\\n==> Calculando mAP final no conjunto de validação...\")\n\npred_boxes, true_boxes = get_evaluation_bboxes(\n    val_loader, model, iou_threshold=NMS_IOU_THRESH, anchors=ANCHORS, \n    threshold=CONF_THRESHOLD, device=DEVICE\n)\nmap_val = mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=MAP_IOU_THRESH, num_classes=NUM_CLASSES\n)\n\nprint(f\"\\n\" + \"=\"*30)\nprint(f\"  RESULTADO FINAL DO TREINAMENTO\")\nprint(f\"  mAP no conjunto de validação: {map_val:.4f}\")\nprint(\"=\"*30)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}